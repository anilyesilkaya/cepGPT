{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75dddc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4f7ba",
   "metadata": {},
   "source": [
    "# Character level N-gram Language Model\n",
    "In the creation of this notebook I was heavily unfluenced by the following sources:\n",
    "\n",
    "[1] https://github.com/karpathy/makemore\n",
    "\n",
    "[2] https://github.com/rain1024/slp2-pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5026b",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "Predicting in general is a difficult task. Especially trying to predict the next word, subword, character or in general a token (piece of text could be as small as a character and as big as the whole corpus). In daily life we have situations where we can predict what's coming next in a sentence...\n",
    "```\n",
    "Could you get some lemons from the ...?\n",
    "```\n",
    "Obviously, in the example above the possibilities are not infinite `[\"shops\", \"Tesco's, \"Some other shop name\", ...]`. How do we make that prediction in our heads? Even better question how can we come up with a system that could do those predictions for us? All those questions are related to the natural language processing (NLP) specically language models (LMs) are machine learning models that predicts the upcoming tokens.\n",
    "\n",
    "Why do we need to predict the next token?\n",
    "1. We can build systems where speech-to-text and text-to-speech could be implemented\n",
    "2. We can build systems where the typos and grammatical mistakes can easily be corrected.\n",
    "3. More importantly, as an inherent part of the language, predicting the next word/sentence/paragraph means the previous information has become a knowledge and accordingly the coherent communication still goes on.\n",
    "\n",
    "In this notebook we will be starting with the simplest kind of language model: the __n-gram__ language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae16f61",
   "metadata": {},
   "source": [
    "## Character Level N-gram Language Models\n",
    "In this section we will both investigate the theoretical and practical implementation of the N-gram language models, which will be working in the character level meaning that our language model's goal will be to predict the __next character__ for the given sequence of text since this will be one of the easiest applications to learn the inner dynamics of language models.\n",
    "\n",
    "The name __gram__ comes from the Greek word grámma (γράμμα), meaning _something written_ or _a small unit_. An `N`-gram represents the sequence of characters i.e. 2-gram (or _bigram_) means 2 character sequence such as $(c_1,~ c_2)$. Similarly, 3-gram (or _trigram_) means 3 character sequence such as $(c_1,~c_2,~c_3)$. The term `N`-gram means that the model is probabilistic and that can estimate the new character based on the `N-1` previous characters $(c_1,~c_2,\\cdots,~c_{N-1})$.\n",
    "\n",
    "In general sense, our task of the prediction of the next chracter prediction can be represented as $P(c~|~h)$, where $c$ and $h$ represent the next character to be predicted and the history of the characters. For instance, we can try to predict the probability of the next character being `\"a\"` knowing that the previous character was `\"n\"` $P(\\text{\"a\"}~|~\\text{\"n\"})$.\n",
    "\n",
    "- One way to estimate the probability $P(\\text{\"a\"}~|~\\text{\"n\"})$ is directly from relative frequency counts, meaning a very large corpus, count the number of times the character `\"n\"` is followed by `\"a\"`. We can answer the question: out of times we saw the history $h$, how many times was it followed by the character $c$.\n",
    "\n",
    "- If we had a sufficiently large corpus, we could compute this count and estimate the probability. However, as the history grows, the number of possible contexts grows exponentially and we need increasingly more data to estimate the next character accurately and coherently, which is not an easy task. Hence, we need a more clever and efficient approach.\n",
    "\n",
    "Let's define some notation:\n",
    "\n",
    "Throughout this notebook we will use character as tokens. To represent the probability of a particular random variable $X_i$ taking the value `\"n\"` represented as $P(X_i = \\text{\"n\"}) = P(\\text{\"n\"})$.\n",
    "\n",
    "We will represent the sequence of $n$ characters $(c_1,~c_2,~\\cdots,~c_n)$ will be represented by the notation $(c_{1:n})$. Note that the order of the elements in this notation is important.\n",
    "\n",
    "Also, the joint probability of each chracter in a sequence having a particular value $P(X_1 = c_1,~X_2=c_2,~\\cdots,~X_n=c_n)$ will be represented by $P(c_{1:n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ea4f7",
   "metadata": {},
   "source": [
    "### Calculating the Probabilities of Entire Sequences\n",
    "How can we compute the probabilities of the entire sequences, $(c_1,~c_2,~\\cdots,~c_n)$, $P(c_1 \\cap c_2 \\cap \\cdots \\cap c_n)$? The simplest thing we can do to use the **Bayes' theorem** to recursively calculate the joint probabilities. Bayes' theorem for drawing a single card from deck example could be applied to as follows:\n",
    "\\begin{equation*}\n",
    "P(\\text{Ace} \\cap \\text{black}) = \\frac{P(\\text{drawing a black Ace})}{P(\\text{drawing a black card})} = \\frac{P(\\text{Ace} \\cap \\text{black})}{P(\\text{black})}\n",
    "\\label{eq:1} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"images/bayes_theorem.png\" alt=\"Bayes' theorem\" width=\"450\"/>\n",
    "</p>\n",
    "\n",
    "#### Simple/Intuitive Example\n",
    "Let's simplify the task and try to understand what's going on. Let's focus on how to calculate the same probability for 3 characters? What is the probability of the characters $(c_1,~c_2,~c_3)$ occuring sequentially?\n",
    "\\begin{equation*}\n",
    "P(\\text{the sequence} ~(c_1,~c_2,~c_3)~ \\text{occurring in this order}) = P(c_1\\cap c_2 \\cap c_3) = ~?\n",
    "\\label{eq:2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "We already know that we can use Bayes' theorem to express the joint probability of $(c_1,~c_2,~c_3)$ occuring in terms of the probability that the character $c_3$ appearing after the sequence $(c_1,~c_2)$\n",
    "\\begin{equation*}\n",
    "P(c_3~|~ c_1 \\cap c_2) = \\frac{P(c_1\\cap c_2\\cap c_3)}{P(c_1 \\cap c_2)} \\quad \\text{or}\\quad P(c_1\\cap c_2\\cap c_3) = P(c_3~|~ c_1 \\cap c_2)\\times P(c_1 \\cap c_2)\n",
    "\\label{eq:3} \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "Similarly, we can ask what is the probability of the character $c_2$ occuring after $c_1$?\n",
    "\\begin{equation*}\n",
    "P(c_2~|~c_1) = \\frac{P(c_2 \\cap c_1)}{P(c_1)} \\quad \\text{or}\\quad P(c_2 \\cap c_1) = P(c_2~|~c_1)\\times P(c_1)\n",
    "\\label{eq:4} \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "If we combine the expressions $\\eqref{eq:2}$, $\\eqref{eq:3}$ and $\\eqref{eq:4}$,\n",
    "\\begin{equation*}\n",
    "P(c_1\\cap c_2 \\cap c_3) = P(c_3~|~ c_1 \\cap c_2)\\times P(c_2~|~c_1)\\times P(c_1)\n",
    "\\label{eq:5} \\tag{5}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78207d",
   "metadata": {},
   "source": [
    "Let's expand the same example into a case where we would like to calculate the probability of $(c_1,~c_2,~c_3,~c_4)$ exactly in the given order:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Step 1:}\\quad &P(c_1\\cap c_2 \\cap c_3 \\cap c_4) = P(c_4~|~ c_1 \\cap c_2 \\cap c_3)\\times P(c_1 \\cap c_2 \\cap c_3),\\\\\n",
    "\\text{Step 2:}\\quad &P(c_1 \\cap c_2 \\cap c_3) = P(c_3~|~ c_1 \\cap c_2)\\times P(c_1 \\cap c_2),\\\\\n",
    "\\text{Step 3:}\\quad &P(c_1 \\cap c_2) = P(c_2~|~ c_1)\\times P(c_1)\n",
    "\\end{align*}\n",
    "\n",
    "Now, if we combine all the steps together\n",
    "\n",
    "\\begin{align*}\n",
    "&P(c_1\\cap c_2 \\cap c_3 \\cap c_4) = P(c_4~|~ c_1 \\cap c_2 \\cap c_3)\\times P(c_3~|~ c_1 \\cap c_2)\\times P(c_2~|~ c_1)\\times P(c_1)\n",
    "\\label{eq:6} \\tag{6}\n",
    "\\end{align*}\n",
    "\n",
    "As can be seen from above there is a nice pattern, which can be simplified as,\n",
    "\n",
    "\\begin{equation*}\n",
    "P(c_1,~c_2,~\\cdots,~c_n) = \\prod_{k=1}^{n} P(c_k~|~ c_{1:k-1})\n",
    "\\label{eq:7} \\tag{7}\n",
    "\\end{equation*}\n",
    "\n",
    "The above expression gives us the link between the joint probabilty of a particular sequence and computing the conditional probability of the next character given previous characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b59fbc",
   "metadata": {},
   "source": [
    "- Please note from the above expression that the we can calculate the probability of a particular sequence occuring, $P(c_1,~c_2,~\\cdots,~,c_n)$ as long as if we can calculate the conditional probability, $P(c_k~|~ c_{1:k-1})$ for each $k$. Unfortunately, we don't have any way to calculate the expression $P(c_k~|~ c_{1:k-1})$.\n",
    "\n",
    "- As an additional note we can't just count the number of occurences of each character sequence with varying length since the number of possible combinations for a character sequence with lengh $N$ ($N$-gram) becomes $30^N$, since we will see shortly that we have $30$ characters in our Turkish names prediction alphabet, which becomes infeasible too quickly due to the highly exponential nature. Moreover, some particular sequences might have not been occured in the training corpus before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03786f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = np.arange(0, 10)\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "plt.plot(N, 30**N)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"$30^{N}$\")\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0848264",
   "metadata": {},
   "source": [
    "### The Markov Assumption\n",
    "The intiutiton of an $N$-gram model is that instead of computing the probability of the next character being, $c$, given its entire history, $h$, we can **approximate** the history by using just a few last characters. In other words, instead of solving the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(c_1,~c_2,~\\cdots,~c_n) = \\prod_{k=1}^{n} P(c_k~|~ c_{1:k-1})\n",
    "\\label{eq:8} \\tag{8}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098986a",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "We can try to make the following approximation:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(c_1,~c_2,~\\cdots,~c_n) = \\prod_{k=1}^{n} P(c_k~|~ c_{k-1})\n",
    "\\label{eq:9} \\tag{9}\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, we can approximate the probability $P(c_k~|~ c_{1:k-1})$, which is really hard to compute we can just use $P(c_k~|~ c_{k-1})$, where we know that we can calculate $P(c_k~|~ c_{k-1})$ much easier by just counting the bigrams ($N=2$) in the corpus.\n",
    "\n",
    "#### Unigram Model\n",
    "We can go one more notch further and assume that no history is involved in our decisions,\n",
    "\\begin{equation*}\n",
    "P(c_1,~c_2,~\\cdots,~c_n) = \\prod_{k=1}^{n} P(c_k)\n",
    "\\label{eq:10} \\tag{10}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Simple Example\n",
    "**Markov assumption** or **Markov property** can be explained with the following example:\n",
    "\n",
    "Assume we have a jar contains two red (2xR) and one a green (G) ball.\n",
    "<p align=\"left\">\n",
    "  <img src=\"images/markov_assumption.png\" alt=\"Bayes' theorem\" width=\"250\"/>\n",
    "</p>\n",
    "\n",
    "- One ball was drawn yesterday.\n",
    "- One ball was drawn today.\n",
    "- The final ball will be drawn tomorrow. All the draws are **without replacement**.\n",
    "\n",
    "Suppose you know that today's ballwas **red**, but yout have no information about yesterday's ball. What can we infer about the tomorrow's ball $P(\\text{drawing } R \\text{ tomorrow})$?\n",
    "| Today | Tomorrow     |\n",
    "|-------|--------------|\n",
    "|   R   | $P(R) = 0.5$ |\n",
    "|   G   | $P(R) = 1$   |\n",
    "\n",
    "If we knew what has been drawn yesterday, how would $P(\\text{drawing } R \\text{ tomorrow})$ change?\n",
    "\n",
    "| Yesterday   | Today | Tomorrow    |\n",
    "|-------------|-------|-------------|\n",
    "| If it was R |   R   | $P(R) = 0$  |\n",
    "| If it was R |   G   | $P(R) = 1$  |\n",
    "| If it was G |   R   | $P(R) = 1$  |\n",
    "\n",
    "The experiment above shows that the probability distribution for **tomorrow's color not only depends on the present value but is also information about the past**. The probability density function (pdf) is changing when we have more information about yesterdays draw. In order to understand underlying mathematics we will need to understand conditional probabilities.\n",
    "\n",
    "##### Bayes' Theorem\n",
    "Let's try to calculate the probability of drawing an ace and a card which is black:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{Ace}) &= \\frac{\\text{number of aces}}{\\text{number of cards}} = 4/52 = 1/13 \\\\ \\\\\n",
    "P(\\text{Black}) &= \\frac{\\text{number of blacks}}{\\text{number of cards}} =  13/52 = 1/4 \\\\ \\\\\n",
    "P(\\text{Black and Ace}) &= P(\\text{Ace and Black}) = \\frac{\\text{number of black Aces}}{\\text{number of cards}} = 1/52\n",
    "\\label{eq:11} \\tag{11}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a00f8",
   "metadata": {},
   "source": [
    "How can we calculate the following probability? $P(\\text{Ace given that we already have a black card})$? \n",
    "\n",
    "Since we know that we have a black card:\n",
    "\\begin{equation*}\n",
    "P(\\text{Ace given we've drawn a black card}) = P(\\text{Ace}~|~\\text{black}) = \\frac{\\text{number of black aces}}{\\text{number of blacks}} = \\frac{1}{13}\n",
    "\\label{eq:12} \\tag{12}\n",
    "\\end{equation*}\n",
    "The above expression $(\\text{number of black aces})/(\\text{number of blacks})$ could also be calculated using $\\eqref{eq:12}$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{Ace given we've drawn a black card}) = \\frac{\\text{number of black aces}}{\\text{number of blacks}} = \\frac{P(\\text{Black and Ace})\\times (\\text{number of cards})}{P(\\text{Black})\\times(\\text{number of cards})} = \\frac{P(\\text{Black and ace})}{P(\\text{Black})}\n",
    "\\end{align*}\n",
    "Thus,\n",
    "\\begin{align*}\n",
    "P(\\text{Ace}~|~\\text{Black}) = \\frac{P(\\text{Black and ace})}{P(\\text{Black})}\n",
    "\\label{eq:13} \\tag{13}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf4d0b-8eda-4c8e-a9d5-fe9e79f4120a",
   "metadata": {},
   "source": [
    "#### The Markov Property Continues\n",
    "Since we've learnt about the conditional probabilities and Bayes' theorem. We can continue investigating the Markov property and related probabilities.\n",
    "<p align=\"left\">\n",
    "  <img src=\"images/markov_assumption_2.png\" alt=\"Bayes' theorem\" width=\"350\"/>\n",
    "</p>\n",
    "We know that if the experiment was done with replacement, the probabilities would be equal for tomorrow's draw, where today's and yesterday's draws would be irrelevant. In other words, draws would be independent and Markov property would be satisfied.\n",
    "\\begin{align*}\n",
    "&P(G\\text{ tomorrow}) = 1/3\\\\\n",
    "&P(R\\text{ tomorrow}) = 1/3 + 1/3 = 2/3\n",
    "\\end{align*}\n",
    "\n",
    "However, when we have an experiment without replacement, draws are not independent and the Markov property is not getting satisfied. The above diagram summarizes the jar with three balls without replacement experiment. We can also express the probabilities using the conditional probabilities as follows:\n",
    "\\begin{align*}\n",
    "&P(R\\text{ tomorrow}~|~(R\\text{ yesterday},~R\\text{ today}) = 0\\\\\n",
    "&P(R\\text{ tomorrow}~|~(R\\text{ yesterday},~G\\text{ today})) = 1\\\\\n",
    "&P(R\\text{ tomorrow}~|~(G\\text{ yesterday},~R\\text{ today}) =1\\\\ \\\\\n",
    "&P(R\\text{ today}~|~R\\text{ yesterday}) = 1/2 \\\\\n",
    "&P(R\\text{ today}~|~G\\text{ yesterday}) = 1 \\\\ \\\\\n",
    "&P(R\\text{ tomorrow}) = 2/3 \\\\\n",
    "&P(G\\text{ tomorrow}) = 1/3\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e579bd-0acc-4c4a-a6b9-3645f26ed90c",
   "metadata": {},
   "source": [
    "#### N-gram Model\n",
    "\n",
    "It is important to emphasize that even though the probability of some character sequence occuring is completely dependent on the whole _history_ for the simplification and approximation purposes we can assume that Markov property holds and only information we need is only $N-1$ steps in $N$-gram models. For instance, for a bigram model ($N=2$) with $n=5$ characters becomes, \n",
    "\\begin{align*}\n",
    "P(c_1,~c_2,~c_3,~c_4,~c_5) &= P\\big(c_5~|~(c_1,~c_2,~c_3,~c_4)\\big)\\times P\\big(c_4~|~(c_1,~c_2,~c_3)\\big)\\times P\\big(c_3~|~(c_1,~c_2)\\big)\\times P(c_2~|~c_1)\\times P(c_1) \\\\\n",
    "&\\approx P(c_5~|~c_4)\\times P(c_4~|~c_3)\\times P(c_3~|~c_2)\\times P(c_2~|~c_1)\\times P(c_1)\n",
    "\\label{eq:14} \\tag{14}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a11939-f784-45d9-b192-2cb81c59506d",
   "metadata": {},
   "source": [
    "Let's do another example for $n=5$ and $N=3$,\n",
    "\\begin{align*}\n",
    "P(c_1,~c_2,~c_3,c_4,~c_5) \\approx P\\big(c_5~|~(c_3,~c_4)\\big)\\times P\\big(c_4~|~(c_2,~c_3)\\big)\\times P(c_2~|~c_1)\\times P(c_1)  \n",
    "\\label{eq:15}\\tag{15}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9dd88-0693-4ecc-a160-39a61eca930a",
   "metadata": {},
   "source": [
    "If we generalize the expressions \\eqref{eq:14} and \\eqref{eq:15},\n",
    "\\begin{align*}\n",
    "P(c_1,~c_2,c_3,~c_4,\\cdots,~c_n) \\approx \\prod_{k=1}^{n}P(c_k~|~c_{k-N+1:k-1})\n",
    "\\label{eq:15} \\tag{16}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f34db-7df8-42fb-9b8c-fd00caec6eaa",
   "metadata": {},
   "source": [
    "##### How to Estimate the Probabilities?\n",
    "Now, we've obtained a simpler and more computable version of the \\eqref{eq:8} in \\eqref{eq:15}. However, another issue still exists. How will we calculate the probabilities $P(c_k~|~c_{k-N+1:k-1}$ or $P\\big(c_k~|~(c_{k-2},~c_{k-1})\\big)$ for $N=2$?\n",
    "\n",
    "An intuitive way to estimate the probabilities is the *maximum likelihoode estimation (MLE)*. Accordingly, we can get the MLE estimates of the parameters if the $N$-gram model is the getting the counts from the corpus, and **normalizing** the counts so that they lie between $0$ and $1$. In other words, once normalized the counts could be used as the **probability distributions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37cbf2-3f16-452b-9f93-f0d7bd2b1dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
